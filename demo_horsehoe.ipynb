{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesbridge import BayesBridge, RegressionModel, RegressionCoefPrior\n",
    "from bayesbridge import HorseshoePrior\n",
    "from simulate_data import simulate_design, simulate_outcome\n",
    "from util import mcmc_summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesBridge supports both dense (numpy array) and sparse (scipy sparse matrix) design matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs, n_pred = 10 ** 4, 10 ** 3\n",
    "\n",
    "X = simulate_design(\n",
    "    n_obs, n_pred, \n",
    "    binary_frac=.9,\n",
    "    binary_pred_freq=.2,\n",
    "    shuffle_columns=True,\n",
    "    format_='sparse',\n",
    "    seed=111\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_true = np.zeros(n_pred)\n",
    "beta_true[:5] = 1.5\n",
    "beta_true[5:10] = 1.\n",
    "beta_true[10:15] = .5\n",
    "\n",
    "n_trial = np.ones(X.shape[0]) # Binary outcome.\n",
    "y = simulate_outcome(\n",
    "    X, beta_true, intercept=0., \n",
    "    n_trial=n_trial, model='logit', seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horseshoe prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionModel(\n",
    "    y, X, family='logit',\n",
    "    add_intercept=True, center_predictor=True,\n",
    "        # Do *not* manually add intercept to or center X.\n",
    ")\n",
    "\n",
    "prior = HorseshoePrior(\n",
    "    n_fixed_effect=0,\n",
    "    sd_for_intercept=float('inf'),\n",
    "    sd_for_fixed_effect=1,\n",
    "    regularizing_slab_size=2.,\n",
    "    skew_mean=0.,\n",
    "    skew_sd=1.,\n",
    "    global_scale_prior=None\n",
    ")\n",
    "\n",
    "bridge = BayesBridge(model, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Gibbs sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, mcmc_info = bridge.gibbs(\n",
    "    n_iter=250, n_burnin=0, thin=1, \n",
    "    init={'global_scale': .01},\n",
    "    coef_sampler_type='cg',\n",
    "    seed=111\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check convergence by looking at the traceplot for posterior log-density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "plt.plot(samples['logp'])\n",
    "plt.xlabel('MCMC iteration')\n",
    "plt.ylabel('Posterior log density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart MCMC from the last iteration with 'gibbs_resume()'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, mcmc_info = bridge.gibbs_resume(\n",
    "    mcmc_info, n_add_iter=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "plt.plot(samples['logp'])\n",
    "plt.xlabel('MCMC iteration')\n",
    "plt.ylabel('Posterior log density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more samples (while keeping the previous ones) with 'merge=True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, mcmc_info = bridge.gibbs_resume(\n",
    "    mcmc_info, n_add_iter=750, merge=True, prev_samples=samples\n",
    ")\n",
    "coef_samples = samples['coef'][1:, :] # Extract all but the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "plt.plot(samples['logp'])\n",
    "plt.xlabel('MCMC iteration')\n",
    "plt.ylabel('Posterior log density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check mixing of regression coefficients and their posterior marginals.\n",
    "\n",
    "Typically the convergence is quick and mixing of the regression coefficients is adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "plt.plot(coef_samples[[0, 5, 10, 15], :].T)\n",
    "plt.xlabel('MCMC iteration')\n",
    "plt.ylabel(r'$\\beta_j$', rotation=0, labelpad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "n_coef_to_plot = 25\n",
    "\n",
    "mcmc_summarizer.plot_conf_interval(\n",
    "    coef_samples, conf_level=.95, \n",
    "    n_coef_to_plot=n_coef_to_plot, marker_scale=1.4\n",
    ");\n",
    "plt.plot(\n",
    "    beta_true[:n_coef_to_plot], '--', color='tab:orange',\n",
    "    label='True value'\n",
    ")\n",
    "plt.title('Horseshoe')\n",
    "plt.xlabel(r'Coefficient index $j$')\n",
    "plt.ylabel(r'$\\beta_j$', rotation=0, labelpad=10)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the ExpTiltedStableDist class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesbridge.random.tilted_stable import ExpTiltedStableDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still unable to use the exp()\n",
    "from bayesbridge.random.tilted_stable import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge_exponent = 1. / 16\n",
    "divide_conquer_cost = 10 ** np.linspace(-1., 1., 101)\n",
    "\n",
    "char_exponent = bridge_exponent / 2\n",
    "tilt = divide_conquer_cost ** (1. / char_exponent)\n",
    "# For Bayesian bridge, tilt parameter is given by beta / global_scale\n",
    "tilt_power = tilt ** char_exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time the samplers at given parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tilted_stable = ExpTiltedStableDist(seed=0)\n",
    "\n",
    "def time_method(char_exponent, tilt, method, n_rep=1000):\n",
    "    start = time.time()\n",
    "    tilted_stable.sample(\n",
    "        char_exponent * np.ones(n_rep), tilt * np.ones(n_rep), \n",
    "        method=method\n",
    "    );\n",
    "    elapsed = time.time() - start\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repetition = 1000\n",
    "\n",
    "exec_time = {\n",
    "    method: \n",
    "        np.array([\n",
    "            time_method(char_exponent, tilt_i, method, n_repetition)\n",
    "            for tilt_i in tilt\n",
    "        ]) \n",
    "    for method in ['double-rejection', 'divide-conquer']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4.5))\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "for method in ['double-rejection', 'divide-conquer']:\n",
    "    plt.plot(tilt_power, exec_time[method] / n_repetition)\n",
    "plt.xlabel('Cost of divide-conquer (= tilt ^ char-exponent)')\n",
    "plt.ylabel('Sec. per sample')\n",
    "plt.ticklabel_format(axis='y', scilimits=(0,0))\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "for side in ['top', 'right']:\n",
    "    plt.gca().spines[side].set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
